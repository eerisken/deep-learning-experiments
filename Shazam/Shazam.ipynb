{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12898910,"sourceType":"datasetVersion","datasetId":8161440}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install torchaudio qdrant-client --quiet\n\n# Download metadata and audio\n!wget -nc https://os.unil.cloud.switch.ch/fma/fma_small.zip -O fma_small.zip\n#!wget -nc https://os.unil.cloud.switch.ch/fma/fma_metadata.zip -O fma_metadata.zip\n\n# Unzip (audio: fma_small, metadata: CSVs)\n!unzip -q -n fma_small.zip -d ./fma_small\n#!unzip -q -n fma_metadata.zip -d ./fma_metadata","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YrZAgqpKL6z","outputId":"7757c77a-4a56-4b81-d9c0-8998298ae66e","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:23:25.928227Z","iopub.execute_input":"2025-08-28T16:23:25.928808Z","iopub.status.idle":"2025-08-28T16:23:34.578860Z","shell.execute_reply.started":"2025-08-28T16:23:25.928771Z","shell.execute_reply":"2025-08-28T16:23:34.577891Z"}},"outputs":[{"name":"stdout","text":"File ‘fma_small.zip’ already there; not retrieving.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ntracks = pd.read_csv(\"fma_metadata/fma_metadata/tracks.csv\", index_col=0, header=[0,1])\ngenres = pd.read_csv(\"fma_metadata/fma_metadata/genres.csv\", index_col=0)\n\n# Filter only small set\nsubset = tracks['set', 'subset'] == 'small'\nsmall_tracks = tracks[subset]\n\nprint(\"Total FMA-Small tracks:\", len(small_tracks))\nsmall_tracks.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":631},"id":"t9cPElVpKbUU","outputId":"072b7cbe-d836-4f9e-f9bb-73614b371dba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total FMA-Small tracks: 8000\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"small_tracks"},"text/html":["\n","  <div id=\"df-6141928e-6b19-4070-8717-1bf0afc13a88\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"10\" halign=\"left\">album</th>\n","      <th>...</th>\n","      <th colspan=\"10\" halign=\"left\">track</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>comments</th>\n","      <th>date_created</th>\n","      <th>date_released</th>\n","      <th>engineer</th>\n","      <th>favorites</th>\n","      <th>id</th>\n","      <th>information</th>\n","      <th>listens</th>\n","      <th>producer</th>\n","      <th>tags</th>\n","      <th>...</th>\n","      <th>information</th>\n","      <th>interest</th>\n","      <th>language_code</th>\n","      <th>license</th>\n","      <th>listens</th>\n","      <th>lyricist</th>\n","      <th>number</th>\n","      <th>publisher</th>\n","      <th>tags</th>\n","      <th>title</th>\n","    </tr>\n","    <tr>\n","      <th>track_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2008-11-26 01:44:45</td>\n","      <td>2009-01-05 00:00:00</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>&lt;p&gt;&lt;/p&gt;</td>\n","      <td>6073</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>4656</td>\n","      <td>en</td>\n","      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n","      <td>1293</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>Food</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>2008-11-26 01:44:45</td>\n","      <td>2009-01-05 00:00:00</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>&lt;p&gt;&lt;/p&gt;</td>\n","      <td>6073</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>1933</td>\n","      <td>en</td>\n","      <td>Attribution-NonCommercial-ShareAlike 3.0 Inter...</td>\n","      <td>1151</td>\n","      <td>NaN</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>This World</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>2008-11-26 01:45:08</td>\n","      <td>2008-02-06 00:00:00</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>47632</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>54881</td>\n","      <td>en</td>\n","      <td>Attribution-NonCommercial-NoDerivatives (aka M...</td>\n","      <td>50135</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>Freeway</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>1</td>\n","      <td>2008-11-26 01:49:59</td>\n","      <td>2007-05-22 00:00:00</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>61</td>\n","      <td>&lt;p&gt;Alec K. Redfearn &amp;amp; The Eyesores: Ellen ...</td>\n","      <td>1300</td>\n","      <td>Alec K. Refearn, Rob Pemberton</td>\n","      <td>[]</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>1593</td>\n","      <td>en</td>\n","      <td>Attribution-Noncommercial-No Derivative Works ...</td>\n","      <td>1299</td>\n","      <td>NaN</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>Queen Of The Wires</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>0</td>\n","      <td>2008-11-26 01:49:57</td>\n","      <td>2009-01-16 00:00:00</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>&lt;p&gt;A full ensamble of strings, drums, electron...</td>\n","      <td>1304</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>839</td>\n","      <td>en</td>\n","      <td>Attribution-Noncommercial-No Derivative Works ...</td>\n","      <td>725</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>Ohio</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 52 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6141928e-6b19-4070-8717-1bf0afc13a88')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6141928e-6b19-4070-8717-1bf0afc13a88 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6141928e-6b19-4070-8717-1bf0afc13a88');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f38d8763-2a99-4e9e-a50d-2522a73fa65b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f38d8763-2a99-4e9e-a50d-2522a73fa65b')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f38d8763-2a99-4e9e-a50d-2522a73fa65b button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["            album                                                     \\\n","         comments         date_created        date_released engineer   \n","track_id                                                               \n","2               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n","5               0  2008-11-26 01:44:45  2009-01-05 00:00:00      NaN   \n","10              0  2008-11-26 01:45:08  2008-02-06 00:00:00      NaN   \n","140             1  2008-11-26 01:49:59  2007-05-22 00:00:00      NaN   \n","141             0  2008-11-26 01:49:57  2009-01-16 00:00:00      NaN   \n","\n","                                                                           \\\n","         favorites  id                                        information   \n","track_id                                                                    \n","2                4   1                                            <p></p>   \n","5                4   1                                            <p></p>   \n","10               4   6                                                NaN   \n","140              1  61  <p>Alec K. Redfearn &amp; The Eyesores: Ellen ...   \n","141              1  60  <p>A full ensamble of strings, drums, electron...   \n","\n","                                                       ...       track  \\\n","         listens                        producer tags  ... information   \n","track_id                                               ...               \n","2           6073                             NaN   []  ...         NaN   \n","5           6073                             NaN   []  ...         NaN   \n","10         47632                             NaN   []  ...         NaN   \n","140         1300  Alec K. Refearn, Rob Pemberton   []  ...         NaN   \n","141         1304                             NaN   []  ...         NaN   \n","\n","                                 \\\n","         interest language_code   \n","track_id                          \n","2            4656            en   \n","5            1933            en   \n","10          54881            en   \n","140          1593            en   \n","141           839            en   \n","\n","                                                                              \\\n","                                                    license listens lyricist   \n","track_id                                                                       \n","2         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1293      NaN   \n","5         Attribution-NonCommercial-ShareAlike 3.0 Inter...    1151      NaN   \n","10        Attribution-NonCommercial-NoDerivatives (aka M...   50135      NaN   \n","140       Attribution-Noncommercial-No Derivative Works ...    1299      NaN   \n","141       Attribution-Noncommercial-No Derivative Works ...     725      NaN   \n","\n","                                                    \n","         number publisher tags               title  \n","track_id                                            \n","2             3       NaN   []                Food  \n","5             6       NaN   []          This World  \n","10            1       NaN   []             Freeway  \n","140           2       NaN   []  Queen Of The Wires  \n","141           4       NaN   []                Ohio  \n","\n","[5 rows x 52 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"code","source":"SAMPLE_RATE = 16000\nN_MELS = 64\nCLIP_DURATION = 10     # seconds (small for training batches, large enough for music context)\nN_SAMPLES = SAMPLE_RATE * CLIP_DURATION","metadata":{"id":"1YCirQ6F4ftj","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:23:52.726962Z","iopub.execute_input":"2025-08-28T16:23:52.727211Z","iopub.status.idle":"2025-08-28T16:23:52.731668Z","shell.execute_reply.started":"2025-08-28T16:23:52.727183Z","shell.execute_reply":"2025-08-28T16:23:52.730814Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nfrom tqdm import tqdm\nimport glob\n\n#DATASET_PATH = \"fma_small/fma_small\"\n#CACHE_PATH = \"/content/fma_cache\"\nDATASET_PATH = \"/kaggle/working/fma_small/fma_small\"\nCACHE_PATH   = \"/kaggle/working/fma_cache\"\nos.makedirs(CACHE_PATH, exist_ok=True)\n\n# Parameters\nclip_duration = CLIP_DURATION       # seconds\nsample_rate = SAMPLE_RATE\nn_mels = N_MELS\nn_samples = clip_duration * sample_rate\n\n# Mel transform\nmel_transform = torchaudio.transforms.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=1024,\n    hop_length=512,\n    n_mels=n_mels\n)\n\n# Gather all song paths\nsong_files = [p for p in glob.glob(os.path.join(DATASET_PATH, \"*/*.mp3\"))]\nsong_dirs = {f\"song_{i:06d}\": p for i, p in enumerate(song_files)}\n\n# Precompute\nfor song_id, path in tqdm(song_dirs.items(), desc=\"Caching spectrograms\"):\n    cache_file = os.path.join(CACHE_PATH, f\"{song_id}.pt\")\n    if os.path.exists(cache_file):\n        continue  # skip if already cached\n\n    try:\n        waveform, sr = torchaudio.load(path)\n    except Exception as e:\n        print(f\"Skipping problematic file {song_id}: {e}\")\n        continue  # skip problematic file\n    if waveform.size(1) < 1000:\n      print(f\"Skipping too short file: {song_id}\")\n      continue\n    if sr != sample_rate:\n        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)  # mono\n\n    # Take random clip_duration seconds (or pad if short)\n    if waveform.size(1) < n_samples:\n        pad = n_samples - waveform.size(1)\n        waveform = torch.nn.functional.pad(waveform, (0, pad))\n    else:\n        max_start = waveform.size(1) - N_SAMPLES\n        start = torch.randint(0, max_start + 1, (1,)).item()\n        waveform = waveform[:, start:start + N_SAMPLES]\n\n    mel = mel_transform(waveform)\n    log_mel = torch.log1p(mel)\n    log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n    log_mel = log_mel.contiguous()\n\n    torch.save(log_mel, cache_file)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJYDOrqRZKQA","outputId":"201a27f6-d42e-4fc1-d329-7fa12176f520","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport random\nimport os\n\nclass SongTripletDatasetCached(Dataset):\n    def __init__(self, cache_dir, clip_duration=5, sample_rate=16000, hop_length=512):\n        \"\"\"\n        clip_duration: seconds of each anchor/positive/negative\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.cache_files = [os.path.join(cache_dir, f) for f in os.listdir(cache_dir)]\n        self.song_ids = [os.path.basename(f).split(\".pt\")[0] for f in self.cache_files]\n\n        # Compute number of mel frames per clip\n        self.frames_per_sec = sample_rate / hop_length\n        self.n_frames = int(clip_duration * self.frames_per_sec)\n\n    def __len__(self):\n        # You can use the number of cached songs, or any large number for random triplet sampling\n        return 5 * len(self.song_ids)\n\n    def random_crop(self, mel):\n        total_frames = mel.size(1)\n        if total_frames <= self.n_frames:\n            return mel  # too short, return as is\n        start = random.randint(0, total_frames - self.n_frames)\n        return mel[:, start:start + self.n_frames]\n\n    def __getitem__(self, idx):\n        # --- Anchor ---\n        anchor_id = random.choice(self.song_ids)\n        anchor_mel = torch.load(os.path.join(self.cache_dir, f\"{anchor_id}.pt\"))\n        anchor = self.random_crop(anchor_mel)\n\n        # --- Positive (same song, different clip) ---\n        positive_mel = torch.load(os.path.join(self.cache_dir, f\"{anchor_id}.pt\"))\n        positive = self.random_crop(positive_mel)\n\n        # --- Negative (different song) ---\n        neg_id = random.choice([s for s in self.song_ids if s != anchor_id])\n        negative_mel = torch.load(os.path.join(self.cache_dir, f\"{neg_id}.pt\"))\n        negative = self.random_crop(negative_mel)\n\n        return anchor, positive, negative\n","metadata":{"id":"n_e-0luDRapm","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:37:24.303178Z","iopub.execute_input":"2025-08-28T16:37:24.303842Z","iopub.status.idle":"2025-08-28T16:37:24.312195Z","shell.execute_reply.started":"2025-08-28T16:37:24.303815Z","shell.execute_reply":"2025-08-28T16:37:24.311436Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os\n\n# ----------------------------\n# Embedding Network\n# ----------------------------\n\"\"\"\nclass AudioEmbeddingNet(nn.Module):\n    def __init__(self, embedding_dim=128):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        self.fc = nn.Linear(128, embedding_dim)\n\n    def forward(self, x):\n        # x: (B, 1, n_mels, time)\n        x = self.conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return F.normalize(x, p=2, dim=1)  # L2-normalized embeddings\n\"\"\"\n\nclass AudioEmbeddingNet(nn.Module):\n    def __init__(self, n_mels=64, embedding_dim=128, lstm_hidden=128, lstm_layers=1):\n        super(AudioEmbeddingNet, self).__init__()\n\n        # ------------------------------\n        # 1️⃣ CNN feature extractor\n        # ------------------------------\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(3,3), padding=(1,1)),  # input: (B,1,n_mels,T)\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2,2)),  # downsample (n_mels//2, T//2)\n\n            nn.Conv2d(32, 64, kernel_size=(3,3), padding=(1,1)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2,2)),  # downsample further\n\n            nn.Conv2d(64, 128, kernel_size=(3,3), padding=(1,1)),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n\n        # ------------------------------\n        # 2️⃣ BiLSTM for temporal modeling\n        # ------------------------------\n        # After CNN, we collapse the mel-dim, keep time as sequence\n        self.lstm = nn.LSTM(\n            input_size=128*(n_mels//4),  # depends on pooling\n            hidden_size=lstm_hidden,\n            num_layers=lstm_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # ------------------------------\n        # 3️⃣ Fully connected embedding\n        # ------------------------------\n        self.fc = nn.Linear(2*lstm_hidden, embedding_dim)  # BiLSTM -> embedding\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, 1, n_mels, T) log-mel spectrogram\n        \"\"\"\n        B, C, H, W = x.size()\n        x = self.cnn(x)  # (B, 128, H', W')\n        x = x.permute(0, 3, 1, 2)  # (B, W', C, H')\n        x = x.contiguous().view(B, x.size(1), -1)  # flatten mel+channels -> seq_len x features\n\n        lstm_out, _ = self.lstm(x)  # (B, seq_len, 2*lstm_hidden)\n        # Take mean over time dimension\n        embedding = lstm_out.mean(dim=1)\n        embedding = self.fc(embedding)  # final embedding\n        embedding = F.normalize(embedding, p=2, dim=1)  # optional: normalize\n\n        return embedding\n\n# Triplet wrapper\nclass TripletNetwork(nn.Module):\n    def __init__(self, embedding_net):\n        super().__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, anchor, positive, negative):\n        return (self.embedding_net(anchor),\n                self.embedding_net(positive),\n                self.embedding_net(negative))\n","metadata":{"id":"6hpt1UKwLO96","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:37:42.064644Z","iopub.execute_input":"2025-08-28T16:37:42.064925Z","iopub.status.idle":"2025-08-28T16:37:42.074666Z","shell.execute_reply.started":"2025-08-28T16:37:42.064902Z","shell.execute_reply":"2025-08-28T16:37:42.073926Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nimport os\n\n# 🔹 1. Train/Val split\ndef create_loaders(dataset, batch_size=32, val_split=0.2):\n    val_size = int(len(dataset) * val_split)\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n\n    return train_loader, val_loader\n\n\n# 🔹 2. Training loop with early stopping + scheduler\ndef train_model(model, train_loader, val_loader, n_epochs=50, patience=5, save_path=\"best_model.pt\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = model.to(device)\n\n    criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n\n    best_val_loss = np.inf\n    patience_counter = 0\n\n    for epoch in range(1, n_epochs + 1):\n        # --- Training ---\n        model.train()\n        total_train_loss = 0\n        for anchor, positive, negative in train_loader:\n            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n\n            optimizer.zero_grad()\n\n            anchor_out, positive_out, negative_out = model(anchor, positive, negative)\n\n            loss = criterion(anchor_out, positive_out, negative_out)\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n\n        # --- Validation ---\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for anchor, positive, negative in val_loader:\n                anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n                anchor_out, positive_out, negative_out = model(anchor, positive, negative)\n                val_loss = criterion(anchor_out, positive_out, negative_out)\n                total_val_loss += val_loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n\n        # 🔹 Scheduler step\n        scheduler.step(avg_val_loss)\n\n        print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        # --- Check early stopping ---\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), save_path)  # save best model\n            print(f\"  ✅ New best model saved (val_loss={best_val_loss:.4f})\")\n        else:\n            patience_counter += 1\n            print(f\"  ⚠️ No improvement (patience {patience_counter}/{patience})\")\n\n        if patience_counter >= patience:\n            print(\"⏹️ Early stopping triggered\")\n            break\n\n    # Load best model before returning\n    model.load_state_dict(torch.load(save_path))\n    print(\"🔄 Best model reloaded from checkpoint\")\n    return model\n","metadata":{"id":"YGLltewwOcwb","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:37:48.990911Z","iopub.execute_input":"2025-08-28T16:37:48.991193Z","iopub.status.idle":"2025-08-28T16:37:49.001911Z","shell.execute_reply.started":"2025-08-28T16:37:48.991172Z","shell.execute_reply":"2025-08-28T16:37:49.001212Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nimport glob\nfrom torch.utils.data import DataLoader, random_split\n\ndataset = SongTripletDatasetCached(CACHE_PATH)\nval_split = 0.1\nval_size = int(len(dataset) * val_split)\ntrain_size = len(dataset) - val_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n\n# Model\nmodel = TripletNetwork(AudioEmbeddingNet(embedding_dim=128))\n\n# If more than 1 GPU is available, wrap with DataParallel\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel = model.cuda()  # move to GPU(s)\n\n# Train\nbest_model = train_model(model, train_loader, val_loader, n_epochs=20, patience=5, save_path=\"triplet_best.pt\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heQFrOHSO2hg","outputId":"f08d56e0-d92a-4dca-c6a1-36cf54df028a","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T16:37:56.771203Z","iopub.execute_input":"2025-08-28T16:37:56.771512Z","iopub.status.idle":"2025-08-28T17:20:15.794556Z","shell.execute_reply.started":"2025-08-28T16:37:56.771488Z","shell.execute_reply":"2025-08-28T17:20:15.793564Z"}},"outputs":[{"name":"stdout","text":"Epoch 001 | Train Loss: 0.0086 | Val Loss: 0.0043 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0043)\nEpoch 002 | Train Loss: 0.0026 | Val Loss: 0.0061 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 003 | Train Loss: 0.0021 | Val Loss: 0.0023 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0023)\nEpoch 004 | Train Loss: 0.0015 | Val Loss: 0.0012 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0012)\nEpoch 005 | Train Loss: 0.0017 | Val Loss: 0.0024 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 006 | Train Loss: 0.0014 | Val Loss: 0.0014 | LR: 0.001000\n  ⚠️ No improvement (patience 2/5)\nEpoch 007 | Train Loss: 0.0011 | Val Loss: 0.0010 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0010)\nEpoch 008 | Train Loss: 0.0009 | Val Loss: 0.0011 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 009 | Train Loss: 0.0010 | Val Loss: 0.0006 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0006)\nEpoch 010 | Train Loss: 0.0007 | Val Loss: 0.0008 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 011 | Train Loss: 0.0005 | Val Loss: 0.0005 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0005)\nEpoch 012 | Train Loss: 0.0006 | Val Loss: 0.0006 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 013 | Train Loss: 0.0006 | Val Loss: 0.0005 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0005)\nEpoch 014 | Train Loss: 0.0005 | Val Loss: 0.0007 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 015 | Train Loss: 0.0004 | Val Loss: 0.0003 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0003)\nEpoch 016 | Train Loss: 0.0003 | Val Loss: 0.0002 | LR: 0.001000\n  ✅ New best model saved (val_loss=0.0002)\nEpoch 017 | Train Loss: 0.0005 | Val Loss: 0.0003 | LR: 0.001000\n  ⚠️ No improvement (patience 1/5)\nEpoch 018 | Train Loss: 0.0003 | Val Loss: 0.0008 | LR: 0.001000\n  ⚠️ No improvement (patience 2/5)\nEpoch 019 | Train Loss: 0.0003 | Val Loss: 0.0003 | LR: 0.000500\n  ⚠️ No improvement (patience 3/5)\nEpoch 020 | Train Loss: 0.0002 | Val Loss: 0.0001 | LR: 0.000500\n  ✅ New best model saved (val_loss=0.0001)\n🔄 Best model reloaded from checkpoint\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport os\n\n# Constants\nSAMPLE_RATE = 16000\nHOP_LENGTH = 512\nN_MELS = 64\nCROP_DURATION = 5  # seconds for random crop\nCROP_FRAMES = int(CROP_DURATION * SAMPLE_RATE / HOP_LENGTH)  # frames in mel spectrogram\n\n# Mel spectrogram transform\nmel_transform = torchaudio.transforms.MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=1024,\n    hop_length=512,\n    n_mels=N_MELS\n)\n\ndef preprocess_mp3_file(path, device=\"cpu\"):\n    \"\"\"\n    path: path to an .mp3 file\n    Returns a random 5-second crop of log-mel spectrogram, normalized\n    \"\"\"\n    try:\n        waveform, sr = torchaudio.load(path)\n    except Exception as e:\n        print(f\"Skipping {song_id}: {e}\")\n        return None  \n    if waveform.size(1) < 1000:\n      print(f\"Skipping too short file: {song_id}\")\n      return None\n    if sr != sample_rate:\n        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)  # mono\n\n    mel = mel_transform(waveform)\n\n    # Random crop\n    total_frames = mel.size(1)\n    if total_frames <= CROP_FRAMES:\n        cropped = mel\n    else:\n        start = random.randint(0, total_frames - CROP_FRAMES)\n        cropped = mel[:, start:start + CROP_FRAMES]\n        \n    log_mel = torch.log1p(cropped)\n    log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n    \n    return log_mel.unsqueeze(0).to(device).contiguous()  # (1, n_mels, time)\n    \ndef preprocess_cached_file(path, device=\"cpu\"):\n    \"\"\"\n    path: path to cached .pt file (10-second log-mel spectrogram)\n    Returns a random 5-second crop, normalized\n    \"\"\"\n    # Load cached mel spectrogram\n    mel = torch.load(path)  # (n_mels, time_frames)\n\n    # Random crop\n    total_frames = mel.size(1)\n    if total_frames <= CROP_FRAMES:\n        cropped = mel\n    else:\n        start = random.randint(0, total_frames - CROP_FRAMES)\n        cropped = mel[:, start:start + CROP_FRAMES]\n\n    # Normalize\n    cropped = (cropped - cropped.mean()) / (cropped.std() + 1e-6)\n\n    return cropped.unsqueeze(0).to(device).contiguous()  # (1, n_mels, time)\n\ndef embed_song(embedding_net, path, device=\"cpu\"):\n    \"\"\"\n    embedding_net: your neural net\n    path: path to cached mel .pt file\n    \"\"\"\n    embedding_net.eval()\n    with torch.no_grad():\n        #spec = preprocess_cached_file(path, device)\n        spec = preprocess_mp3_file(path, device)\n        emb = embedding_net(spec)  # (1, embedding_dim)\n        return emb.squeeze(0).cpu().numpy().astype(np.float32)\n","metadata":{"id":"NpypkuiTNLFe","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:35:28.412371Z","iopub.execute_input":"2025-08-28T17:35:28.412971Z","iopub.status.idle":"2025-08-28T17:35:28.423237Z","shell.execute_reply.started":"2025-08-28T17:35:28.412951Z","shell.execute_reply":"2025-08-28T17:35:28.422668Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from qdrant_client import QdrantClient\n\n# Connect to Qdrant\nclient = QdrantClient(\":memory:\")\n\n# Delete collection\nclient.delete_collection(collection_name=\"songs\")\n\nprint(\"✅ Collection 'songs' deleted\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zywICYxpMuY","outputId":"ce8f20ec-5c8a-495c-b3e4-98ebd071c605"},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Collection 'songs' deleted\n"]}],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nfrom tqdm import tqdm\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models as rest\n\n# Qdrant client (local or cloud)\n#client = QdrantClient(host=\"localhost\", port=6333)\nclient = QdrantClient(\":memory:\")  # only works in current runtime, no network needed\n\n# Pick first 1000 MP3s MELs from FMA-Small\n#DATASET_PATH = \"fma_small/fma_small\"\n#CACHE_PATH = \"/content/fma_cache\"\nDATASET_PATH = \"/kaggle/working/fma_small/fma_small\"\nCACHE_PATH   = \"/kaggle/working/fma_cache\"\n#song_files = sorted(glob.glob(os.path.join(CACHE_PATH, \"*/*.pt\")))[:1000]\nsong_files = sorted(glob.glob(os.path.join(DATASET_PATH, \"*/*.mp3\")))[:1000]\n\n# Create or reset collection\nclient.recreate_collection(\n    collection_name=\"songs\",\n    vectors_config=rest.VectorParams(size=128, distance=\"Cosine\")\n)\n\nembedding_net = AudioEmbeddingNet(embedding_dim=128)\n# Load weights from TripletNetwork\nstate_dict = torch.load(\"triplet_best.pt\", map_location=\"cpu\")\n# If keys are prefixed with 'embedding_net.', strip them\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    if k.startswith(\"embedding_net.\"):\n        new_state_dict[k.replace(\"embedding_net.\", \"\")] = v\n\nembedding_net.load_state_dict(new_state_dict)\n\n# Upsert songs\npoints = []\nfor idx, path in enumerate(tqdm(song_files, desc=\"Indexing songs\")):\n    try:\n        emb = embed_song(embedding_net, path, device=\"cpu\")  # use embedding_net here\n        emb = emb / np.linalg.norm(emb)\n        points.append(\n            rest.PointStruct(\n                id=idx,\n                vector=emb.tolist(),\n                payload={\"track\": os.path.basename(path)}\n            )\n        )\n    except Exception as e:\n        print(f\"❌ Failed {path}: {e}\")\n\n# Bulk insert\nif points:\n    client.upsert(\n        collection_name=\"songs\",\n        points=points\n    )\n\nprint(f\"✅ Inserted {len(points)} songs into Qdrant\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dHsPRendNjjS","outputId":"c0acd1b9-2fc6-41ee-c813-ccdd3aee2478","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:35:39.324409Z","iopub.execute_input":"2025-08-28T17:35:39.324684Z","iopub.status.idle":"2025-08-28T17:37:26.642946Z","shell.execute_reply.started":"2025-08-28T17:35:39.324665Z","shell.execute_reply":"2025-08-28T17:37:26.642317Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2078873022.py:20: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n  client.recreate_collection(\nIndexing songs: 100%|██████████| 1000/1000 [01:47<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Inserted 1000 songs into Qdrant\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"query_path = song_files[512]  # pick any > 1000\nquery_emb = embed_song(embedding_net, query_path)\nquery_emb = query_emb / np.linalg.norm(query_emb)\n\nresults = client.search(\n    collection_name=\"songs\",\n    query_vector=query_emb.tolist(),\n    limit=3\n)\n\nprint(\"\\n🔎 Query:\", os.path.basename(query_path))\nfor hit in results:\n    print(f\"Match: {hit.payload['track']} (score={hit.score:.3f})\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5_B3sngNkse","outputId":"501e81a6-51ea-4c3d-e420-cfc231a85d8c","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:37:59.285275Z","iopub.execute_input":"2025-08-28T17:37:59.285993Z","iopub.status.idle":"2025-08-28T17:37:59.429654Z","shell.execute_reply.started":"2025-08-28T17:37:59.285971Z","shell.execute_reply":"2025-08-28T17:37:59.429021Z"}},"outputs":[{"name":"stdout","text":"\n🔎 Query: 011764.mp3\nMatch: 011764.mp3 (score=1.000)\nMatch: 004232.mp3 (score=0.525)\nMatch: 007376.mp3 (score=0.480)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/179024995.py:5: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  results = client.search(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"SONG_NAME = \"freedom.mp3\"\nSONG_PATH = \"/kaggle/input/test-songs/freedom.mp3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:53:06.539784Z","iopub.execute_input":"2025-08-28T17:53:06.540060Z","iopub.status.idle":"2025-08-28T17:53:06.543776Z","shell.execute_reply.started":"2025-08-28T17:53:06.540040Z","shell.execute_reply":"2025-08-28T17:53:06.543090Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from google.colab import drive\nimport shutil\nimport os\n\n# 1️⃣ Mount Google Drive\ndrive.mount('/content/drive')\n\n# 2️⃣ Set the path to your file in Drive\n#drive_file_path = \"/content/drive/MyDrive/freedom.mp3\"\n#local_path = \"/content/freedom.mp3\"\ndrive_file_path = f\"/content/drive/MyDrive/{SONG_NAME}\"\n#local_path = f\"/content/{SONG_NAME}\"\nlocal_path = f\"/kaggle/working/{SONG_NAME}\"\n\n\n# 3️⃣ Copy to Colab working directory\nshutil.copy(drive_file_path, local_path)\n\n# 4️⃣ Check\nassert os.path.exists(local_path), \"File not found!\"\n#print(f\"✅ freedom.mp3 copied to {local_path}\")\nprint(f\"✅ {SONG_NAME} copied to {local_path}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9y6nP6AsvD8","outputId":"fb75fe1b-3eaf-40fb-ad64-167f2767986d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchaudio\nimport torch\nimport numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import PointStruct\n\n#song_path = \"/content/freedom.mp3\"  # path to Beyoncé's Freedom\n#song_path = \"/content/halo.mp3\"  # path to Beyoncé's Halo\ncollection_name = \"songs\"\n\n#embedding_freedom = embed_song(embedding_net, song_path, device=\"cpu\")\nembedding_freedom = embed_song(embedding_net, SONG_PATH, device=\"cpu\")\nembedding_freedom = embedding_freedom / np.linalg.norm(embedding_freedom)  # normalize\n\n# ------------------------------\n# 3️⃣ Connect to Qdrant\n# ------------------------------\nqdrant_client = client\n\n# Only create the collection if it doesn't exist\nif not qdrant_client.collection_exists(collection_name=collection_name):\n    qdrant_client.create_collection(\n        collection_name=collection_name,\n        vectors_config={\"size\": 128, \"distance\": \"Cosine\"}\n    )\n\n# ------------------------------\n# 4️⃣ Upsert Beyoncé's Freedom or Halo\n# ------------------------------\nqdrant_client.upsert(\n    collection_name=collection_name,\n    points=[\n        PointStruct(\n            id=1000000,  # unique ID\n            vector=embedding_freedom.tolist(),\n            payload={\"track\": \"Beyonce Freedom\"}\n            #payload={\"track\": \"Beyonce Halo\"}\n        )\n    ]\n)\n\n#print(\"✅ Inserted 'Beyonce Freedom' into Qdrant\")\nprint(f\"✅ Inserted '{SONG_NAME}' into Qdrant\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJp_4iSQiJm6","outputId":"10a48e8b-1475-41f8-887a-a8498872c914","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:53:33.559589Z","iopub.execute_input":"2025-08-28T17:53:33.559868Z","iopub.status.idle":"2025-08-28T17:53:34.447430Z","shell.execute_reply.started":"2025-08-28T17:53:33.559846Z","shell.execute_reply":"2025-08-28T17:53:34.446564Z"}},"outputs":[{"name":"stdout","text":"✅ Inserted 'freedom.mp3' into Qdrant\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"#query_path = \"/content/freedom.mp3\"\n#query_path = \"/content/halo.mp3\"\nquery_path = SONG_PATH\nquery_emb = embed_song(embedding_net, query_path)\nquery_emb = query_emb / np.linalg.norm(query_emb)\n\nresults = client.search(\n    collection_name=\"songs\",\n    query_vector=query_emb.tolist(),\n    limit=3\n)\n\nprint(\"\\n🔎 Query:\", os.path.basename(query_path))\nfor hit in results:\n    print(f\"Match: {hit.payload['track']} (score={hit.score:.3f})\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4co1e2mtshd","outputId":"b9488ca7-e330-4649-ef80-3d10c79b0051","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T17:53:47.079221Z","iopub.execute_input":"2025-08-28T17:53:47.079514Z","iopub.status.idle":"2025-08-28T17:53:47.977145Z","shell.execute_reply.started":"2025-08-28T17:53:47.079493Z","shell.execute_reply":"2025-08-28T17:53:47.976339Z"}},"outputs":[{"name":"stdout","text":"\n🔎 Query: freedom.mp3\nMatch: Beyonce Freedom (score=1.000)\nMatch: Beyonce Halo (score=0.789)\nMatch: 001642.mp3 (score=0.607)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3672160408.py:7: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  results = client.search(\n","output_type":"stream"}],"execution_count":32}]}